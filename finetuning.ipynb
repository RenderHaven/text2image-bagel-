{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7adcc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "\n",
    "from data.data_utils import add_special_tokens, pil_img2rgb\n",
    "from data.transforms import ImageTransform\n",
    "from inferencer import InterleaveInferencer\n",
    "from modeling.autoencoder import load_ae\n",
    "from modeling.bagel import (\n",
    "    BagelConfig, Bagel,\n",
    "    Qwen2Config, Qwen2ForCausalLM,\n",
    "    SiglipVisionConfig, SiglipVisionModel\n",
    ")\n",
    "from modeling.qwen2 import Qwen2Tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722d1dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"dataset/bloom_vist_story2image.parquet\")\n",
    "print(df.head())\n",
    "\n",
    "df = df.sample(n=100, random_state=42).reset_index(drop=True)\n",
    "print(\"Using rows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a0acaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryImageDataset(Dataset):\n",
    "    def __init__(self, df, image_root):\n",
    "        self.df = df\n",
    "        self.image_root = image_root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        prompt = row[\"text_prompt\"]\n",
    "        image_path = os.path.join(self.image_root, row[\"image_path\"])\n",
    "\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = pil_img2rgb(image)\n",
    "\n",
    "        return {\n",
    "            \"text\": prompt,\n",
    "            \"image\": image\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = \"models/BAGEL-7B-MoT\"\n",
    "CHECKPOINT = os.path.join(MODEL_PATH, \"ema.safetensors\")\n",
    "OUTPUT_DIR = \"checkpoints/bagel_finetune\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7c71e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = Qwen2Config.from_json_file(os.path.join(MODEL_PATH, \"llm_config.json\"))\n",
    "llm_config.qk_norm = True\n",
    "llm_config.tie_word_embeddings = False\n",
    "llm_config.layer_module = \"Qwen2MoTDecoderLayer\"\n",
    "\n",
    "vit_config = SiglipVisionConfig.from_json_file(os.path.join(MODEL_PATH, \"vit_config.json\"))\n",
    "vit_config.rope = False\n",
    "vit_config.num_hidden_layers -= 1\n",
    "\n",
    "vae_model, vae_config = load_ae(os.path.join(MODEL_PATH, \"ae.safetensors\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f110fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BagelConfig(\n",
    "    visual_gen=True,\n",
    "    visual_und=False,\n",
    "    llm_config=llm_config,\n",
    "    vit_config=vit_config,\n",
    "    vae_config=vae_config,\n",
    ")\n",
    "\n",
    "with init_empty_weights():\n",
    "    llm = Qwen2ForCausalLM(llm_config)\n",
    "    vit = SiglipVisionModel(vit_config)\n",
    "    model = Bagel(llm, vit, config)\n",
    "\n",
    "model = load_checkpoint_and_dispatch(\n",
    "    model,\n",
    "    checkpoint=CHECKPOINT,\n",
    "    device_map={\"\": \"cuda\"},\n",
    "    dtype=torch.bfloat16,\n",
    ").train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b074e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    if \"language_model\" in name and \"attn\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Trainable params:\", trainable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d47b83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Qwen2Tokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer, new_token_ids, _ = add_special_tokens(tokenizer)\n",
    "\n",
    "vae_transform = ImageTransform(1024, 512, 16)\n",
    "vit_transform = ImageTransform(980, 224, 14)\n",
    "\n",
    "dataset = StoryImageDataset(df, image_root=\"dataset/images\")\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc62b4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=1e-5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536dac19",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 3\n",
    "SAVE_EVERY = 20\n",
    "\n",
    "step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in loader:\n",
    "        step += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        result = model(\n",
    "            text=batch[\"text\"],\n",
    "            image=batch[\"image\"],\n",
    "            return_loss=True\n",
    "        )\n",
    "\n",
    "        loss = result[\"loss\"]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 5 == 0:\n",
    "            print(f\"Epoch {epoch} | Step {step} | Loss {loss.item():.4f}\")\n",
    "\n",
    "        if step % SAVE_EVERY == 0:\n",
    "            ckpt_path = os.path.join(OUTPUT_DIR, f\"step_{step}.pt\")\n",
    "            torch.save(\n",
    "                {\n",
    "                    \"model\": model.state_dict(),\n",
    "                    \"optimizer\": optimizer.state_dict(),\n",
    "                    \"step\": step\n",
    "                },\n",
    "                ckpt_path\n",
    "            )\n",
    "            print(\"ðŸ’¾ Saved checkpoint:\", ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e39607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), os.path.join(OUTPUT_DIR, \"final_model.pt\"))\n",
    "print(\"âœ… Training finished\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
